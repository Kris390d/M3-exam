---
title: "Test"
author: "Andreas Methling"
date: "26/11/2021"
output: pdf_document
---

```{r}
library(readr)
library(tidyverse)


data_start <- read_csv("C:/Users/Simon ik mig/Downloads/lyrics-data.csv.zip") #Simon
artists_data <- read_csv("C:/Users/Simon ik mig/Downloads/artists-data (1).csv")# Simon 

```


```{r}
artists = artists_data %>% 
  group_by(Artist) %>% 
  count(Genre) %>% 
  pivot_wider(names_from = Genre, values_from = n) %>% 
  replace_na(list(Pop = 0, "Hip Hop" = 0, Rock = 0, "Funk Carioca" = 0, "Sertanejo" = 0, Samba = 0 )) %>% 
  ungroup() %>% 
  left_join(artists_data, by = c("Artist")) %>% 
  select(-c(Genre, Genres, Popularity)) %>% 
  distinct()
```


```{r}
glimpse(data_start)

data = data_start %>% 
  filter(Idiom == "ENGLISH") %>% 
  rename("Link" = "ALink") %>% 
  inner_join(artists, by = c("Link")) %>% 
  distinct() %>%
  mutate(name = paste(Artist, SName))%>%
  rename(text=Lyric) %>%
  filter(Rock==1) %>%
  select(name, text)%>%
  distinct(name, .keep_all = T)


data %>%
  count(name, sort = T)

```



# Make labels 

```{r}
library(tidytext)
text_tidy = data %>% unnest_tokens(word, text, token = "words")

head(text_tidy)
```

```{r}
text_tidy %<>%
  filter(str_length(word) > 2 ) %>% 
  group_by(word) %>%
  ungroup() %>%
  anti_join(stop_words, by = 'word') 
```

```{r}
library(hunspell)
text_tidy %>%
  mutate(stem = hunspell_stem(word)) %>%
  unnest(stem) %>%
  count(stem, sort = TRUE)


text_tidy %<>% 
  mutate(stem = hunspell_stem(word)) %>%
  unnest(stem) %>%
   select(-word) %>%
  rename(word = stem)



```

```{r}
top_10000_words=text_tidy %>%
  count(word,sort = T) %>%
  head(10000) %>%
  select(word)

data_top_10000=top_10000_words %>%
  left_join(text_tidy, by= c("word")) 

```

# nrc multiclass

```{r}
sentiment_nrc <- text_tidy %>%  
  inner_join(get_sentiments("nrc"))

multi_data=sentiment_nrc %>%
  filter(sentiment %in% c("negative", "positive", "joy", "fear")) %>%
  count(name, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(label= pmax(positive, joy, negative, fear)) %>%
  mutate(label= ifelse(label == fear, "fear", ifelse(label == positive, "positive", ifelse(label == negative, "negative", ifelse(label == joy, "joy","none label")))))  %>%
  select(name, label) %>%
  inner_join(data)


multi_data_new=sentiment_nrc %>%
  filter(sentiment %in% c("trust", "sadness", "joy", "fear")) %>%
  count(name, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(label= pmax(trust, joy, sadness, fear)) %>%
  mutate(label= ifelse(label == fear, "fear", ifelse(label == trust, "trust", ifelse(label == sadness, "sadness", ifelse(label == joy, "joy","none label")))))  %>%
  select(name, label) %>%
  rename(y= label)%>%
  inner_join(data)

multi_data_new %>%
  count(y)


multi_data %>%
  count(label)
```


# bing
```{r}


sentiment_bing= data_top_10000 %>%
  inner_join(get_sentiments("bing")) %>%
  mutate(sentiment= ifelse(sentiment == "positive", 1,0)) 

sentiment_bing %<>%
  group_by(name) %>%
  summarise(mean= mean(sentiment))%>%
  mutate(label= ifelse(mean>=0.5, 1,0))

```

# Afinn

```{r}
sentiment_afinn= data_top_10000 %>%
  inner_join(get_sentiments("afinn")) 

sentiment_afinn %<>%
  group_by(name) %>%
  summarise(mean= mean(value))%>%
  mutate(label= ifelse(mean>=0, 1,0))

```

# Data

```{r}
data_bing= sentiment_bing %>%
  inner_join(data)%>%
  select(text, label, name)%>%
  rename(y=label)
  

data_afinn= sentiment_afinn %>%
  inner_join(data)%>%
  select(text, label, name)
```
```{r}
library(rsample)

split= initial_split(data_bing, prop = 0.75)

train_data= training(split)
test_data= testing(split)
```


Then we extract our x and y training and test data to be used in the neural networks.
```{r}
x_train_data= train_data %>% pull(text)
y_train_data= train_data %>% pull(y)

x_test_data= test_data %>% pull(text)
y_test_data= test_data %>% pull(y)

head(x_train_data)

```



```{r}
library(keras)
# For Training data
tokenizer <- text_tokenizer(num_words = 10000) %>%                         
  fit_text_tokenizer(x_train_data)                                        
sequences <- texts_to_sequences(tokenizer, x_train_data)                       
one_hot_train <- texts_to_matrix(tokenizer, x_train_data, mode = "binary")   

# For test data

tokenizer <- text_tokenizer(num_words = 10000) %>%                         
  fit_text_tokenizer(x_test_data)                                        
sequences <- texts_to_sequences(tokenizer, x_test_data)                       
one_hot_test <- texts_to_matrix(tokenizer, x_test_data, mode = "binary")
```






```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```


```{r}
model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)
```

```{r}
summary(model)
```

```{r}
history_ann <- model %>% fit(
  one_hot_train,
  y_train_data,
  epochs = 10,
  batch_size = 512,
  validation_split = 0.25
)
```



## Embeding RNN

```{r}
model_emb <- keras_model_sequential() %>%
  layer_embedding(input_dim = 10000, output_dim = 8,            
                  input_length = 300) %>%
  layer_flatten() %>%                                           
  layer_dense(units = 1, activation = "sigmoid")


model_emb %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

summary(model_emb)

history <- model %>% fit(
  one_hot_train, y_train_data,
  epochs = 10,
  batch_size = 32,
  validation_split = 0.2)
```

## LSTM


```{r}
#model <- keras_model_sequential() %>%
  #layer_embedding(input_dim = 10000, output_dim = 32) %>%
  #layer_lstm(units = 32) %>%
  #layer_dense(units = 1, activation = "sigmoid")

#model %>% compile(
  #optimizer = "rmsprop",
  #loss = "binary_crossentropy",
  #metrics = c("acc")
#)

#history <- model %>% fit(
  #one_hot_train, y_train_data,
  #epochs = 3,
  #batch_size = 62,
  #validation_split = 0.2
#)
```



# multiclass model 


## Data preprocess

```{r}
library(rsample)

split_multi= initial_split(multi_data_new, prop = 0.75)

train_data_multi= training(split_multi)
test_data_multi= testing(split_multi)
```


Then we extract our x and y training and test data to be used in the neural networks.
```{r}
x_train_data= train_data_multi %>% pull(text)
y_train_data= train_data_multi %>% select('y', "name") %>% mutate(n= 1) %>% pivot_wider(names_from = y, values_from = n, values_fill = 0) %>% select(-name) %>% as.matrix()

x_test_data= test_data_multi %>% pull(text)
y_test_data= test_data_multi %>% pull(y)



```


```{r}

# For Training data
tokenizer <- text_tokenizer(num_words = 1000) %>%                         
  fit_text_tokenizer(x_train_data)                                        
sequences <- texts_to_sequences(tokenizer, x_train_data)                       
one_hot_train <- texts_to_matrix(tokenizer, x_train_data, mode = "binary")   

# For test data

tokenizer <- text_tokenizer(num_words = 1000) %>%                         
  fit_text_tokenizer(x_test_data)                                        
sequences <- texts_to_sequences(tokenizer, x_test_data)                       
one_hot_test <- texts_to_matrix(tokenizer, x_test_data, mode = "binary")
```

## ANN

```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(1000)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "softmax")
```


```{r}
model %>% compile(
  optimizer = "adam",
  loss = "categorical_crossentropy",
  metrics = "accuracy"
)
```

```{r}
summary(model)
```



```{r}
history_ann <- model %>% fit(
  one_hot_train,
  y_train_data,
  epochs = 10,
  batch_size = 512,
  validation_split = 0.25
)
```
