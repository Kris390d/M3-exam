---
title: "Test"
author: "Andreas Methling"
date: "26/11/2021"
output: pdf_document
---

```{r}
library(readr)
library(tidyverse)


#data_start <- read_csv("C:/Users/Simon ik mig/Downloads/lyrics-data.csv.zip") #Simon
#artists_data <- read_csv("C:/Users/Simon ik mig/Downloads/artists-data (1).csv")# Simon 
data_start <- read_csv("C:/Users/andre/Desktop/lyrics-data.csv")
artists_data <- read_csv("C:/Users/andre/Downloads/artists-data.csv")

```


```{r}
artists = artists_data %>% 
  group_by(Artist) %>% 
  count(Genre) %>% 
  pivot_wider(names_from = Genre, values_from = n) %>% 
  replace_na(list(Pop = 0, "Hip Hop" = 0, Rock = 0, "Funk Carioca" = 0, "Sertanejo" = 0, Samba = 0 )) %>% 
  ungroup() %>% 
  left_join(artists_data, by = c("Artist")) %>% 
  select(-c(Genre, Genres, Popularity)) %>% 
  distinct()
```


```{r}
glimpse(data_start)

data = data_start %>% 
  filter(Idiom == "ENGLISH") %>% 
  rename("Link" = "ALink") %>% 
  inner_join(artists, by = c("Link")) %>% 
  distinct() %>%
  mutate(name = paste(Artist, SName))%>%
  rename(text=Lyric) %>%
  filter(Rock==1 & Pop==1) %>%
  select(name, text)%>%
  distinct(name, .keep_all = T)


data %>%
  count(name, sort = T)

```



# Make labels 

```{r}
library(tidytext)
text_tidy = data %>% unnest_tokens(word, text, token = "words")

head(text_tidy)
```

```{r}
text_tidy %<>%
  filter(str_length(word) > 2 ) %>% 
  group_by(word) %>%
  ungroup() %>%
  anti_join(stop_words, by = 'word') 
```

```{r}
library(hunspell)
text_tidy %>%
  mutate(stem = hunspell_stem(word)) %>%
  unnest(stem) %>%
  count(stem, sort = TRUE)


text_tidy %<>% 
  mutate(stem = hunspell_stem(word)) %>%
  unnest(stem) %>%
   select(-word) %>%
  rename(word = stem)



```

```{r}
top_10000_words=text_tidy %>%
  count(word,sort = T) %>%
  head(10000) %>%
  select(word)

data_top_10000=top_10000_words %>%
  left_join(text_tidy, by= c("word")) 

```

# Bing
```{r}
sentiment_bing= data_top_10000 %>%
  inner_join(get_sentiments("bing")) %>%
  mutate(sentiment= ifelse(sentiment == "positive", 1,0)) 

sentiment_bing %<>%
  group_by(name) %>%
  summarise(mean= mean(sentiment))%>%
  mutate(label= ifelse(mean>=0.5, 1,0))

```

# Afinn

```{r}
sentiment_afinn= data_top_10000 %>%
  inner_join(get_sentiments("afinn")) 

sentiment_afinn %<>%
  group_by(name) %>%
  summarise(mean= mean(value))%>%
  mutate(label= ifelse(mean>=0, 1,0))

```

# Data

```{r}
data_bing= sentiment_bing %>%
  inner_join(data)%>%
  select(text, label, name)
  

data_afinn= sentiment_afinn %>%
  inner_join(data)%>%
  select(text, label, name)
```

# Neural Network Bing

```{r}
data_bing_n= data_bing %>%
  rename( y = label )
```


We start by creating test and training data 

```{r}
library(rsample)

split= initial_split(data_bing_n, prop = 0.75)

train_data= training(split)
test_data= testing(split)
```


```{r}
x_train_data= train_data %>% pull(text)
y_train_data= train_data %>% pull(y)

x_test_data= test_data %>% pull(text)
y_test_data= test_data %>% pull(y)
```

Now it is time to load keras and make some adjustments to the data. The data are tweets so a lot of special characters are removed like # and @'s. And then we tokenize our data as we know from basic machine learning to get like a bag of words from our song lyrics and lastly we create a list where every song has a vector which includes the words as a numerical character if the words contained in the tweets are among the 100000 most used words in the dataset.
```{r}
library(keras)

#for training data
tokenizer_train <- text_tokenizer(num_words = 100000,
                                  filters = "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n" ) %>%
  fit_text_tokenizer(x_train_data)

sequences_train = texts_to_sequences(tokenizer_train, x_train_data)


#For test data

tokenizer_test <- text_tokenizer(num_words = 100000,
                                 filters = "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n" ) %>%
  fit_text_tokenizer(x_test_data)

sequences_test = texts_to_sequences(tokenizer_test, x_test_data)

```

## Baseline model

### One-hot encoding

we use this function Daniel made to vectorize the sequences :)
```{r}
vectorize_sequences <- function(sequences, dimension) {
  results <- matrix(0, nrow = length(sequences), ncol = dimension) 
  for(i in 1:length(sequences)){
    results[i, sequences[[i]]] <- 1 
  }
  return(results)
}
```

we use it on the training and test data

```{r}
x_train <- sequences_train %>% vectorize_sequences(dimension = 100000) 
x_test <- sequences_test %>% vectorize_sequences(dimension = 100000) 

str(x_train[1,])
```
What the above has done to the data is, that every tweet now is a row and every feature/word now is a column and then if the tweets has e.g. word 1 then it would have the value 1 otherwise zero. So we basically now have a matrix of size [2488x100000] [number of song in training set x number of words].

###The model

The above data is then used in our baseline model with an input shape of 100000 because that is the size of our input. Then we run it through two dense "relu" layers which is normal procedure for a baseline model. Lastly we have a dense layer with the output which is of unit 1 and is a "sigmoid" layer which means it returns a value between 0 and 1 as we want, as we wanna figure out if a tweet is fake or real.
```{r}
model_keras <- keras_model_sequential()

model <- model_keras %>%
  layer_dense(units = 32, activation = "relu", input_shape = c(100000)) %>%
  layer_dropout(rate = 0.1) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(rate = 0.1) %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

We use baseline model compiling with optimizer "adam", loss "binary" as we are dealing with a binary case and the metric we wanna maximize is accuracy.
```{r}
model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)
```

Here the structure of the model can be viewed
```{r}
summary(model)
```
And now the model is run 10 times with a batch size of 512
```{r}
history_ann <- model %>% fit(
  x_train,
  y_train_data,
  epochs = 10,
  batch_size = 512,
  validation_split = 0.25
)
```

```{r}
plot(history_ann)
```

## Rnn model with paded data
### Padding
In our first baseline model, we used a document-term matrix as inputs for training, with one-hot-encodings (= dummy variables) for the 10.000 most popular terms. This has a couple of disadvantages. Besides being a very large and sparse vector for every review, as a “bag-of-words”, it did not take the word-order (sequence) into account.

This time, we use a different approach, therefore also need a different input data-structure. We now use pad_sequences() to create a integer tensor of shape (samples, word_indices). However, song vary in length, which is a problem since Keras requieres the inputs to have the same shape across the whole sample. Therefore, we use the maxlen = 300 argument, to restrict ourselves to the first 300 words in every song.

The data is paded
```{r}
x_train_pad <- sequences_train %>% pad_sequences(maxlen=300)
x_test_pad <- sequences_test %>% pad_sequences(maxlen=300)

```


```{r}
glimpse(x_train_pad)
```
Now if the value in e.g. the first column of the first tweet is 0 it means that the first  word in the first tweet is not one of the 100000 most used words and there for our model has no integer for it. If there is an integer e.g. "386" it means that that the 386 most commonly used word is the first word in the tweet.


#### The model

setting up the model we will first use a layer_embedding to compress our initial one-hot-encoding vector of length 10.0000 to a “meaning-vector” (=embedding) of the lower dimensionality of 32. Then we add a layer_simple_rnn on top, and finally a layer_dense for the binary prediction of review sentiment.
```{r}
model_keras2 <- keras_model_sequential()

model_rnn <- model_keras2 %>%
  layer_embedding(input_dim = 100000, output_dim = 32) %>%
  layer_simple_rnn(units = 32, activation = "tanh") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

Here the structure of the model can be seen
```{r}
summary(model_rnn)
```

Again we use a basic setup for binary prediction.
```{r}
model_rnn %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)
```


```{r}
history_rnn <- model_rnn %>% fit(
  x_train_pad, y_train_data,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.25
)
```

```{r, warning=FALSE}
plot(history_rnn) 
```

# Neural Network Afinn
Now we try with our Afinn labeled data

```{r}
data_afinn_n= data_afinn %>%
  rename( y = label )
```


We start by creating test and training data 

```{r}
library(rsample)

split2= initial_split(data_afinn_n, prop = 0.75)

train_data2= training(split2)
test_data2= testing(split2)
```


```{r}
x_train_data2= train_data2 %>% pull(text)
y_train_data2= train_data2 %>% pull(y)

x_test_data2= test_data2 %>% pull(text)
y_test_data2= test_data2 %>% pull(y)
```


Again we tokenize
```{r}
library(keras)

#for training data
tokenizer_train2 <- text_tokenizer(num_words = 100000,
                                  filters = "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n" ) %>%
  fit_text_tokenizer(x_train_data2)

sequences_train2 = texts_to_sequences(tokenizer_train2, x_train_data2)


#For test data

tokenizer_test2 <- text_tokenizer(num_words = 100000,
                                 filters = "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n" ) %>%
  fit_text_tokenizer(x_test_data2)

sequences_test2 = texts_to_sequences(tokenizer_test2, x_test_data2)

```

## Baseline model

### One-hot encoding

One-hot encoding again to get a bag of words
```{r}
x_train2 <- sequences_train2 %>% vectorize_sequences(dimension = 100000) 
x_test2 <- sequences_test2 %>% vectorize_sequences(dimension = 100000) 

str(x_train2[1,])
```

###The model

The above data is then used in our baseline model with an input shape of 10000 because that is the size of our input. Then we run it through two dense "relu" layers which is normal procedure for a baseline model. Lastly we have a dense layer with the output which is of unit 1 and is a "sigmoid" layer which means it returns a value between 0 and 1 as we want, as we wanna figure out if a tweet is fake or real.
```{r}
model_keras3 <- keras_model_sequential()

model2 <- model_keras3 %>%
  layer_dense(units = 32, activation = "relu", input_shape = c(100000)) %>%
  layer_dropout(rate = 0.1) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(rate = 0.1) %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

We use baseline model compiling with optimizer "adam", loss "binary" as we are dealing with a binary case and the metric we wanna maximize is accurary.
```{r}
model2 %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)
```

Here the structure of the model can be viewed
```{r}
summary(model2)
```
And now the model is run 10 times with a batch size of 512
```{r}
history_ann <- model2 %>% fit(
  x_train2,
  y_train_data2,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.25
)
```

```{r}
plot(history_ann)
```

## Rnn model with paded data
### Padding
In our first baseline model, we used a document-term matrix as inputs for training, with one-hot-encodings (= dummy variables) for the 10.000 most popular terms. This has a couple of disadvantages. Besides being a very large and sparse vector for every review, as a “bag-of-words”, it did not take the word-order (sequence) into account.

This time, we use a different approach, therefore also need a different input data-structure. We now use pad_sequences() to create a integer tensor of shape (samples, word_indices). However, tweets vary in length, which is a problem since Keras requieres the inputs to have the same shape across the whole sample. Therefore, we use the maxlen = 100 argument, to restrict ourselves to the first 100 words in every tweet also because a tweet maximally can be 280 characters.

The data is paded
```{r}
x_train_pad2 <- sequences_train2 %>% pad_sequences(maxlen=1000)
x_test_pad2 <- sequences_test2 %>% pad_sequences(maxlen=1000)

```


```{r}
glimpse(x_train_pad2)
```
Now if the value in e.g. the first column of the first tweet is 0 it means that the first  word in the first tweet is not one of the 10000 most used words and there for our model has no integer for it. If there is an integer e.g. "386" it means that that the 386 most commonly used word is the first word in the tweet.


#### The model

setting up the model we will first use a layer_embedding to compress our initial one-hot-encoding vector of length 10.000 to a “meaning-vector” (=embedding) of the lower dimensionality of 32. Then we add a layer_simple_rnn on top, and finally a layer_dense for the binary prediction of review sentiment.
```{r}
model_keras4 <- keras_model_sequential()

model_rnn2 <- model_keras4 %>%
  layer_embedding(input_dim = 100000, output_dim = 32) %>%
  layer_simple_rnn(units = 32, activation = "tanh") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

Here the structure of the model can be seen
```{r}
summary(model_rnn2)
```

Again we use a basic setup for binary prediction.
```{r}
model_rnn2 %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)
```


```{r}
history_rnn2 <- model_rnn2 %>% fit(
  x_train_pad2, y_train_data2,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.25
)
```

```{r, warning=FALSE}
plot(history_rnn2) 
```
