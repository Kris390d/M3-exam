---
title: "Test"
author: "Andreas Methling"
date: "26/11/2021"
output: pdf_document
---

```{r}
library(readr)
library(tidyverse)


#data_start <- read_csv("C:/Users/Simon ik mig/Downloads/lyrics-data.csv.zip") #Simon
#artists_data <- read_csv("C:/Users/Simon ik mig/Downloads/artists-data (1).csv")# Simon 
data_start <- read_csv("C:/Users/andre/Desktop/lyrics-data.csv")
artists_data <- read_csv("C:/Users/andre/Downloads/artists-data.csv")

```

Making the artist data ready for import
```{r}
artists = artists_data %>% 
  group_by(Artist) %>% 
  count(Genre) %>% 
  pivot_wider(names_from = Genre, values_from = n) %>% 
  replace_na(list(Pop = 0, "Hip Hop" = 0, Rock = 0, "Funk Carioca" = 0, "Sertanejo" = 0, Samba = 0 )) %>% 
  ungroup() %>% 
  left_join(artists_data, by = c("Artist")) %>% 
  select(-c(Genre, Genres, Popularity)) %>% 
  distinct()
```


Combining the data and only keeping a column with the lyrics and a combined band nam song name column.
```{r}
glimpse(data_start)

data = data_start %>% 
  filter(Idiom == "ENGLISH") %>% 
  rename("Link" = "ALink") %>% 
  inner_join(artists, by = c("Link")) %>% 
  distinct() %>%
  mutate(name = paste(Artist, SName))%>%
  rename(text=Lyric) %>%
  filter(Rock==1 & Pop==1) %>%
  select(name, text)%>%
  distinct(name, .keep_all = T)


data %>%
  count(name, sort = T)

```



# Make labels 

Our data set didnt contain labels so we made them ourselves by first tokenizing
```{r}
library(tidytext)
text_tidy = data %>% unnest_tokens(word, text, token = "words")

head(text_tidy)
```

We remove stopwords and words less than two words.
```{r}
text_tidy %<>%
  filter(str_length(word) > 2 ) %>% 
  group_by(word) %>%
  ungroup() %>%
  anti_join(stop_words, by = 'word') 
```

Then we stem our words.
```{r}
library(hunspell)
text_tidy %>%
  mutate(stem = hunspell_stem(word)) %>%
  unnest(stem) %>%
  count(stem, sort = TRUE)


text_tidy %<>% 
  mutate(stem = hunspell_stem(word)) %>%
  unnest(stem) %>%
   select(-word) %>%
  rename(word = stem)



```

Then we take the 10000 top words, but our data set after preprocessing only contains 8047 words so we move forward with them
```{r}
top_10000_words=text_tidy %>%
  count(word,sort = T) %>%
  head(10000) %>%
  select(word)

data_top_10000=top_10000_words %>%
  left_join(text_tidy, by= c("word")) 

```

# Bing

Then we make our sentiment labels using the bing dictionary by giving every word a sentiment of either 0 for negative or 1 for positive and then we group by song and summarise the sentiment of every word in a song to get the mean, which then becomes the label of the song.
```{r}
sentiment_bing= data_top_10000 %>%
  inner_join(get_sentiments("bing")) %>%
  mutate(sentiment= ifelse(sentiment == "positive", 1,0)) 

sentiment_bing %<>%
  group_by(name) %>%
  summarise(mean= mean(sentiment))%>%
  mutate(label= ifelse(mean>=0.5, 1,0))

```

# Afinn

We have also made labels for the data using the Afinn dictionary, which give the words a value from -5 to 5 so the words can noow also be very positive or negative. Then we do the same and find the mean of every song to get the labels.
```{r}
sentiment_afinn= data_top_10000 %>%
  inner_join(get_sentiments("afinn")) 

sentiment_afinn %<>%
  group_by(name) %>%
  summarise(mean= mean(value))%>%
  mutate(label= ifelse(mean>=0, 1,0))

```

# Data

```{r}
data_bing= sentiment_bing %>%
  inner_join(data)%>%
  select(text, label, name)
  

data_afinn= sentiment_afinn %>%
  inner_join(data)%>%
  select(text, label, name)
```

# Neural Network Bing

Using our bing data set
```{r}
data_bing_n= data_bing %>%
  rename( y = label )
```


We start by creating test and training data 

```{r}
library(rsample)

split= initial_split(data_bing_n, prop = 0.75)

train_data= training(split)
test_data= testing(split)
```


```{r}
x_train_data= train_data %>% pull(text)
y_train_data= train_data %>% pull(y)

x_test_data= test_data %>% pull(text)
y_test_data= test_data %>% pull(y)
```

Now it is time to load keras and make some adjustments to the data. The data are lyrics so not a lot of special characters are used but we still remove them just to be sure. And then we tokenize our data as we know from basic machine learning to get like a bag of words from our song lyrics and lastly we create a list where every song has a vector which includes the words as a numerical character if the words contained in the tweets are among the 100000 most used words in the data set.
```{r}
library(keras)

#for training data
tokenizer_train <- text_tokenizer(num_words = 5000,
                                  filters = "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n" ) %>%
  fit_text_tokenizer(x_train_data)

sequences_train = texts_to_sequences(tokenizer_train, x_train_data)


#For test data

tokenizer_test <- text_tokenizer(num_words = 5000,
                                 filters = "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n" ) %>%
  fit_text_tokenizer(x_test_data)

sequences_test = texts_to_sequences(tokenizer_test, x_test_data)

```

## Baseline model

### One-hot encoding

we use this function Daniel made to vectorize the sequences :)
```{r}
vectorize_sequences <- function(sequences, dimension) {
  results <- matrix(0, nrow = length(sequences), ncol = dimension) 
  for(i in 1:length(sequences)){
    results[i, sequences[[i]]] <- 1 
  }
  return(results)
}
```

we use it on the training and test data

```{r}
x_train <- sequences_train %>% vectorize_sequences(dimension = 5000) 
x_test <- sequences_test %>% vectorize_sequences(dimension = 5000) 

str(x_train[1,])
```
What the above has done to the data is, that every tweet now is a row and every feature/word now is a column and then if the tweets has e.g. word 1 then it would have the value 1 otherwise zero. So we basically now have a matrix of size [2488x5000] [number of song in training set x number of words].

### The model

The above data is then used in our baseline model with an input shape of 5000 because that is the size of our input. Then we run it through two dense "relu" layers which is normal procedure for a baseline model. Lastly we have a dense layer with the output which is of unit 1 and is a "sigmoid" layer which means it returns a value between 0 and 1 as we want, as we wanna figure out if a song is positive or negative.
```{r}
model_keras <- keras_model_sequential()

model <- model_keras %>%
  layer_dense(units = 128, activation = "relu", input_shape = c(5000)) %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

We use baseline model compiling with optimizer "adam", loss "binary" as we are dealing with a binary case and the metric we wanna maximize is accuracy.
```{r}
model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)
```

Here the structure of the model can be viewed, where it can be seen that the model has 656769 tunable parameters, so not the biggest of models but not the smallest either.
```{r}
summary(model)
```
And now the model is run 10 times with a batch size of 256
```{r}
set.seed(12345)
history_ann <- model %>% fit(
  x_train,
  y_train_data,
  epochs = 10,
  batch_size = 256,
  validation_split = 0.25
)
```
We then plot the result of the model
```{r}
plot(history_ann)
```

The top graph shows the lose of the model, where the blue line is the loss of the validation set, which initially falls a bit but then it rises back up. The lower graph shows the same, that the moment the loss rises in the validation set the accuracy falls again. The training set does a lot better than the validation set, which is a indicator of, that our model is over fitted.

Running our model on our test data also shows a bad result
```{r}
metrics = model %>% evaluate(x_test, y_test_data); metrics
```

We will now try to tune the model to get a better result and prevent the over fitting.

### Model tunning

We introduce some dropout layers and reduce the weights of each layer to minimize the number of parameters in the model to prevent the overfitting we saw above.
```{r}
model_keras <- keras_model_sequential()

model2 <- model_keras %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(5000)) %>% 
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 1, activation = "sigmoid")
```

We use baseline model compiling with optimizer "adam", loss "binary" as we are dealing with a binary case and the metric we wanna maximize is accuracy.
```{r}
model2 %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)
```

Here the structure of the model can be viewed, where it can be seen that the model has 656769 tunable parameters, so not the biggest of models but not the smallest either.
```{r}
summary(model2)
```
And now the model is run 10 times with a batch size of 512, so a bigger batch size than the baseline model.
```{r}
set.seed(12345)
history_ann2 <- model2 %>% fit(
  x_train,
  y_train_data,
  epochs = 15,
  batch_size = 512,
  validation_split = 0.25
)
```
We then plot the result of the tunned model
```{r}
plot(history_ann2)
```
The model we have tried to tune seems better as we can see the loss function both of the validation data and the training data continues to go down after every epoch. The accuracy also increases in every period until it stalls out a bit but still this model looks better than the baseline model.


```{r}
metrics2 = model2 %>% evaluate(x_test, y_test_data); metrics2
```
The accuracy of this model is better than the baseline model, but with an accuracy of 59% it is still not any good.


## Rnn model with paded data
### Padding
In our first baseline model, we used a document-term matrix as inputs for training, with one-hot-encodings (= dummy variables) for the 10.000 most popular terms. This has a couple of disadvantages. Besides being a very large and sparse vector for every review, as a “bag-of-words”, it did not take the word-order (sequence) into account.

This time, we use a different approach, therefore also need a different input data-structure. We now use pad_sequences() to create a integer tensor of shape (samples, word_indices). However, song vary in length, which is a problem since Keras requieres the inputs to have the same shape across the whole sample. Therefore, we use the maxlen = 300 argument, to restrict ourselves to the first 300 words in every song.

The data is paded
```{r}
x_train_pad <- sequences_train %>% pad_sequences(maxlen=300)
x_test_pad <- sequences_test %>% pad_sequences(maxlen=300)

```


```{r}
glimpse(x_train_pad)
```
Now if the value in e.g. the first column of the first tweet is 0 it means that the first  word in the first tweet is not one of the 100000 most used words and there for our model has no integer for it. If there is an integer e.g. "386" it means that that the 386 most commonly used word is the first word in the tweet.


#### The model

setting up the model we will first use a layer_embedding to compress our initial one-hot-encoding vector of length 5000 to a “meaning-vector” (=embedding) of the lower dimensionality of 32. Then we add a layer_simple_rnn on top, and finally a layer_dense for the binary prediction of review sentiment.
```{r}
model_keras2 <- keras_model_sequential()

model_rnn <- model_keras2 %>%
  layer_embedding(input_dim = 5000, output_dim = 32) %>%
  layer_simple_rnn(units = 32, activation = "tanh") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

Here the structure of the model can be seen
```{r}
summary(model_rnn)
```

Again we use a basic setup for binary prediction.
```{r}
model_rnn %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)
```

And run our model
```{r}
set.seed(12345)
history_rnn <- model_rnn %>% fit(
  x_train_pad, y_train_data,
  epochs = 10,
  batch_size = 516,
  validation_split = 0.25
)
```

```{r, warning=FALSE}
plot(history_rnn) 
```

Again the traning set outperforms the validation set a lot, which shows our model is overfitted. Further we see that the loss of the validation set starts to climb after a couple of epochs and the accuracy to fall, so not a good model.

```{r}
metrics3 = model_rnn %>% evaluate(x_test_pad, y_test_data); metrics3
```
Running our model on the test data shows an accuracy of 52%, but we will now try to fine tune it to make it better.

#### Tunning the model

This time we again try to reduce the number of parameters to prevent over fitting. We also make another rnn layer and drop a fraction of the units with a drop_out input. Return_sequences = TRUE return the full state sequence of the first rnn layer so next rnn layer gets the full sequence of the input.xMethl
```{r}
model_keras2 <- keras_model_sequential()

model_rnn2 <- model_keras2 %>%
  layer_embedding(input_dim = 5000, output_dim = 16) %>%
  layer_simple_rnn(units = 16, return_sequences = TRUE, activation = "tanh",recurrent_dropout=0.1) %>%
  layer_simple_rnn(units = 16, return_sequences = FALSE, activation = "tanh", recurrent_dropout=0.1) %>%
  layer_dense(units = 1, activation = "sigmoid")
```

Here the structure of the model can be seen
```{r}
summary(model_rnn2)
```

Again we use a basic setup for binary prediction.
```{r}
model_rnn2 %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)
```

And run our model
```{r}
set.seed(12345)
history_rnn2 <- model_rnn2 %>% fit(
  x_train_pad, y_train_data,
  epochs = 25,
  batch_size = 516,
  validation_split = 0.25
)
```


```{r, warning=FALSE}
plot(history_rnn2) 
```

The loss of the traning and validation data seems to follow each other quiet well for the first couple of epochs. After that the validation set begin to vary a lot flying up and down. The accuracy also follows each other a lot, but after around 12 epocs they cross and the traning data runs of. 

```{r}
metrics4 = model_rnn2 %>% evaluate(x_test_pad, y_test_data); metrics4
```
Running our model on the test data shows an accuracy of 58%, this is better than the baseline model but now at all good.


## LTSM

We will now try our data on a LSTM model, but here we are only running one model, since it takes a very long time to run it. We start by using an embeding layer and then we go to a LSTM layer with a unit size of our paded data, which have the size of 300 due to it being the 300 first words in every song. In the lstm layer, we freeze some of the input weigths and also some of the state weights and since we only use one layer we set sequence = false, so it just compiles the input to a single output.
```{r}
model_lstm <- keras_model_sequential() %>%
  layer_embedding(input_dim = 5000, output_dim = 32) %>%
  layer_lstm(units = 300, dropout = 0.25, recurrent_dropout = 0.25, return_sequences = FALSE) %>%
  layer_dense(units = 1, activation = "sigmoid")
```

We use a base compiling
```{r}
model_lstm %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = c("acc")
)
```

The model has a lot of parameters which makes it time consumeing to run it.
```{r}
summary(model_lstm)
```

Then we run the mode
```{r}
history_lstm <- model_lstm %>% fit(
  x_train_pad, y_train_data,
  epochs = 10,
  batch_size = 512,
  validation_split = 0.25
)

```

```{r, warning=FALSE}
plot(history_lstm) 
```

The modeldoesnt seem to perform any better than the previous ones, but the traning and validation data did follow each other quiet well for some epochs, but then at the end they split up due to the increasing loss value of the validation data.

```{r}
metrics5 = model_lstm %>% evaluate(x_test_pad, y_test_data); metrics5
```
A rly poor result to say the least with an accuracy of 47%
